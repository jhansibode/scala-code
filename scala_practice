import org.apache.spark.sql.SparkSession
val spark:SparkSession =SparkSession.builder().master("Local[3]").appName("spark by examples").getOrCreate()
val rdd = sc.textFile("/FileStore/tables/test.txt")
val rdd1 = rdd.flatMap(f=>f.split(","))
val rdd2 = rdd1.map(f=>(f,1))
val rdd3 = rdd2.filter(a=>a._1.startsWith("a"))
val rdd4 = rdd2.reduceByKey(_+_)
rdd4.foreach(println)
val rdd5 = rdd4.map(a=>(a._2,a._1)).sortByKey()
rdd5.count()
val firstrec = rdd5.first()
val maxrec = rdd5.max()
println("count:" +rdd5.count())
rdd5.saveAsTextFile("/FileStore/tables/wordcount")
